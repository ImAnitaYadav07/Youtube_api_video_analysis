# -*- coding: utf-8 -*-
"""youtube-api-eda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FFBR7oAo2jBCasihV0pe6AFnbzWNQu_Z
"""

pip install --upgrade google-api-python-client

! pip install parse

from googleapiclient.discovery import build
import pandas as pd
from IPython.display import JSON
from dateutil import parser


# Data viz packages
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker


# NLP
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
from wordcloud import WordCloud

api_key = 'AIzaSyARWsXghDOj0KDyxCn7uQxJd9IW2ZzHHUw'

channel_ids = ['UCoOae5nYA7VqaXzerajD0lg',
              #more channel here
              ]

# API crediential

api_service_name = "youtube"
api_version = "v3"

# Get credentials and create an API client
youtube = build(api_service_name, api_version, developerKey=api_key)

'''request = youtube.channels().list(
    part="snippet,contentDetails,statistics",
    id=','.join(channel_ids)
    )
response = request.execute()

JSON(response)'''

# get channel statistics

def get_channel_stats(youtube,channel_ids):

  all_data = []

  request = youtube.channels().list(
    part="snippet,contentDetails,statistics",
    id=','.join(channel_ids)
    )

  response = request.execute()

  #loop through items

  for item in response['items']:
    data = {'channelName': item['snippet']['title'],
            'subscribers': item['statistics']['subscriberCount'],
            'views': item['statistics']['viewCount'],
            'totalVideos': item['statistics']['videoCount'],
            'playlistID': item['contentDetails']['relatedPlaylists']['uploads']
            }

    all_data.append(data)

  return (pd.DataFrame(all_data))

channel_stats = get_channel_stats(youtube,channel_ids)

channel_stats

# get video ids


playlistID = 'UUoOae5nYA7VqaXzerajD0lg'

def get_video_ids(youtube,playlistID):

  videosID = []

  request = youtube.playlistItems().list(
        part="snippet,contentDetails",
        playlistId= playlistID,
        maxResults = 50

    )

  response = request.execute()

  for item in response['items']:
    videosID.append(item['contentDetails']['videoId'])

  nxt_page_token = response.get('nextPageToken')
  while nxt_page_token is not None:
    request = youtube.playlistItems().list(
        part="snippet,contentDetails",
        playlistId= playlistID,
        maxResults = 50,
        pageToken = nxt_page_token
    )
    response = request.execute()

    for item in response['items']:
      videosID.append(item['contentDetails']['videoId'])

    nxt_page_token = response.get('nextPageToken')



  return videosID

videosID = get_video_ids(youtube,playlistID)

len(videosID)

# get videos details sub-category

request = youtube.videos().list(
        part="snippet,contentDetails,statistics",
        id=videosID[0:5]
    )
response = request.execute()

JSON(response)

# get video details

def get_video_details(youtube,videosID):

  all_video_info = []

  for i in range(0,len(videosID),50):
    request = youtube.videos().list(
          part="snippet,contentDetails,statistics",
          id= ','.join(videosID[i:i+50])

    )
    response = request.execute()

    for video in response['items']:
      stats_to_keep = {'snippet' : ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],
                       'statistics' : ['viewCount', 'likeCount', 'favoriteCount', 'commentCount'],
                       'contentDetails' : ['duration', 'defination', 'caption']
                       }
      video_info = {}
      video_info['video_id'] = video['id']


      for k in stats_to_keep.keys():
        for v in stats_to_keep[k]:
          try:
            video_info[v] = video[k][v]

          except:
            video_info[v] = None

      all_video_info.append(video_info)

  return(pd.DataFrame(all_video_info))

video_df = get_video_details(youtube,videosID)
video_df

# Get comment details


def get_comment_details(youtube,videosID):

  all_comments = []

  for video_id in videosID:
    try:
      request = youtube.commentThreads().list(
            part="snippet,replies",
            videoId = video_id

      )
      response = request.execute()

      comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items']]
      comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}

      all_comments.append(comments_in_video_info)

    except:
      print('Could not get comments for video ' + video_id)

  return(pd.DataFrame(all_comments))

comment_df = get_comment_details(youtube,videosID)

comment_df.head()

comment_df['comments'][0]

"""# Data Pre-Processing"""

# null filters

video_df.isnull().any()

video_df.dtypes

# Change columns into numeric

numeric_cols = ['viewCount','likeCount','favoriteCount','commentCount']
video_df[numeric_cols] = video_df[numeric_cols].apply(pd.to_numeric, errors = 'coerce', axis = 1)

video_df['viewCount'].dtypes

# Publis day in the week

video_df['publishedAt'] = video_df['publishedAt'].apply(lambda x : parser.parse(x))
video_df['publishDayName'] = video_df['publishedAt'].apply(lambda x: x.strftime("%A"))

! pip install isodate

# convert duration to seconds
import isodate

video_df['durationSecs'] = video_df['duration'].apply(lambda x: isodate.parse_duration(x))
video_df['durationSecs'] = video_df['durationSecs'].astype('timedelta64[s]')

video_df[['durationSecs','duration']]

# Add tags

video_df['tagCount'] = video_df['tags'].apply(lambda x : 0 if x is None else len(x))

video_df

video_df.dtypes

"""# EDA

##  **Best performing videos**
"""

ax = sns.barplot( x='title' ,y='viewCount' ,data = video_df.sort_values('viewCount', ascending = False)[0:9])
plot = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos:'{:,.0f}'.format(x/1000) + 'K'))

"""## **Worst performing videos**"""

ax = sns.barplot(x = 'title', y = 'viewCount', data = video_df.sort_values('viewCount', ascending=True)[0:9])
plot = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos:'{:,.0f}'.format(x/1000) + 'K'))

"""## **View distribution per video**"""

sns.violinplot(x = video_df['channelTitle'], y = video_df['viewCount'])

"""## **Views vs. likes and comments**"""

fig, ax = plt.subplots(1,2)
sns.scatterplot(data = video_df, x = 'commentCount', y = 'viewCount', ax = ax[0])
sns.scatterplot(data = video_df, x = 'likeCount', y = 'viewCount', ax = ax[1])

"""## **Video duration**"""

sns.histplot(data = video_df, x = 'durationSecs', bins=30)

"""## **Wordcloud for video titles**"""

stop_words = set(stopwords.words('english'))
video_df['title_no_stopwords'] = video_df['title'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])

all_words = list([a for b in video_df['title_no_stopwords'].tolist() for a in b])
all_words_str = ' '.join(all_words)

def plot_cloud(wordcloud):
    plt.figure(figsize=(30, 20))
    plt.imshow(wordcloud)
    plt.axis("off");


wordcloud = WordCloud(width = 2000, height = 1000, random_state=1, background_color='black',
                      colormap='viridis', collocations=False).generate(all_words_str)
plot_cloud(wordcloud)

"""## **Upload schedule**"""

day_df = pd.DataFrame(video_df['pushblishDayName'].value_counts())
weekdays = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
day_df = day_df.reindex(weekdays)
ax = day_df.reset_index().plot.bar(x='index', y='pushblishDayName', rot=0)